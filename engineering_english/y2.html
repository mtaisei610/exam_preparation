<!doctype html>
<html lang="ja">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />
    <title>ML Flashcards</title>
    <style>
      :root {
        --primary-color: #007aff; /* iOS Blue */
        --bg-color: #f2f2f7;
        --card-bg: #ffffff;
        --text-color: #000000;
        --secondary-text: #8e8e93;
        --button-bg: #e5e5ea;
      }

      body {
        font-family:
          -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
          Arial, sans-serif;
        background-color: var(--bg-color);
        color: var(--text-color);
        margin: 0;
        padding: 0;
        display: flex;
        flex-direction: column;
        height: 100vh;
        overflow: hidden;
      }

      header {
        background-color: var(--card-bg);
        padding: 1rem;
        text-align: center;
        border-bottom: 1px solid #c6c6c8;
        flex-shrink: 0;
        display: flex;
        flex-direction: column;
        gap: 10px;
      }

      h1 {
        margin: 0;
        font-size: 1.2rem;
        color: var(--text-color);
      }

      .mode-switch {
        display: flex;
        justify-content: center;
        background-color: var(--button-bg);
        border-radius: 8px;
        padding: 2px;
        width: fit-content;
        margin: 0 auto;
      }

      .mode-btn {
        border: none;
        background: none;
        padding: 6px 16px;
        border-radius: 6px;
        font-size: 0.9rem;
        cursor: pointer;
        transition: background-color 0.2s;
        color: var(--text-color);
      }

      .mode-btn.active {
        background-color: var(--card-bg);
        box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        font-weight: bold;
      }

      main {
        flex-grow: 1;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        padding: 20px;
        position: relative;
      }

      .card-container {
        width: 100%;
        max-width: 600px;
        background-color: var(--card-bg);
        border-radius: 20px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
        display: flex;
        flex-direction: column;
        overflow: hidden;
        height: 70vh; /* Fixed height for consistency */
      }

      .question-area {
        flex: 1;
        padding: 30px;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        text-align: center;
        font-size: 1.4rem;
        font-weight: 500;
        border-bottom: 1px dashed #c6c6c8;
        overflow-y: auto;
      }

      .answer-area {
        flex: 1;
        background-color: #f9f9f9;
        cursor: pointer;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        text-align: center;
        padding: 30px;
        font-size: 1.2rem;
        color: var(--text-color);
        transition: background-color 0.2s;
        position: relative;
        user-select: none; /* Prevent text selection on tap */
      }

      .answer-area:active {
        background-color: #eef;
      }

      .answer-placeholder {
        color: var(--secondary-text);
        font-size: 1rem;
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 8px;
      }

      .answer-content {
        display: none;
        width: 100%;
      }

      .answer-area.revealed .answer-placeholder {
        display: none;
      }

      .answer-area.revealed .answer-content {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        animation: fadeIn 0.3s ease;
      }

      .answer-area.revealed {
        background-color: #fff;
        cursor: default;
      }

      @keyframes fadeIn {
        from {
          opacity: 0;
          transform: translateY(5px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .tts-btn {
        background: none;
        border: none;
        cursor: pointer;
        color: var(--primary-color);
        padding: 8px;
        border-radius: 50%;
        margin-left: 8px;
        display: inline-flex;
        align-items: center;
        justify-content: center;
      }

      .tts-btn:active {
        background-color: rgba(0, 122, 255, 0.1);
      }

      .tts-btn svg {
        width: 24px;
        height: 24px;
        fill: currentColor;
      }

      .controls {
        display: flex;
        justify-content: space-between;
        width: 100%;
        max-width: 600px;
        margin-top: 20px;
        padding: 0 10px;
        box-sizing: border-box;
      }

      .nav-btn {
        background-color: var(--card-bg);
        border: 1px solid #c6c6c8;
        color: var(--primary-color);
        padding: 12px 24px;
        border-radius: 12px;
        font-size: 1rem;
        cursor: pointer;
        width: 45%;
        font-weight: 600;
      }

      .nav-btn:active {
        background-color: #f0f0f0;
      }

      .progress {
        margin-top: 10px;
        color: var(--secondary-text);
        font-size: 0.9rem;
      }

      /* Utility for flex centering with icon */
      .text-with-icon {
        display: inline;
      }

      /* Hide TTS button if not relevant language */
      .hidden {
        display: none !important;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>Machine Learning Flashcards</h1>
      <div class="mode-switch">
        <button
          class="mode-btn active"
          id="mode-jp-en"
          onclick="setMode('jp_en')"
        >
          üáØüáµ Êó• ‚Üí Ëã± üá∫üá∏
        </button>
        <button class="mode-btn" id="mode-en-jp" onclick="setMode('en_jp')">
          üá∫üá∏ Ëã± ‚Üí Êó• üáØüáµ
        </button>
      </div>
    </header>

    <main>
      <div class="card-container">
        <div class="question-area" id="question-area"></div>

        <div class="answer-area" id="answer-area" onclick="revealAnswer()">
          <div class="answer-placeholder">
            <span>üëá Tap to reveal</span>
          </div>
          <div class="answer-content" id="answer-content"></div>
        </div>
      </div>

      <div class="progress" id="progress-indicator">1 / 10</div>

      <div class="controls">
        <button class="nav-btn" onclick="prevCard()">‚Üê Prev</button>
        <button class="nav-btn" onclick="nextCard()">Next ‚Üí</button>
      </div>
    </main>

    <script>
      // --- Data Parsing and Setup ---
      // The raw TSV content is embedded here.
      const rawTsv = `
„Éá„Éº„Çø„ÇíË©≥„Åó„ÅèË™ø„Åπ„ÇãÂâç„Å´„ÄÅ„ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Çí‰Ωú„Å£„Å¶Â∞ÅÂç∞„Åó„Å™„Åë„Çå„Å∞„Å™„Çâ„Å™„ÅÑ......„ÄÇ	You should always create a test set and set it aside before inspecting the data closely.
FÂÄ§„ÅØÈÅ©ÂêàÁéá„Å®ÂÜçÁèæÁéá„ÅÆË™øÂíåÂπ≥Âùá„Åß„ÅÇ„Çã„ÄÇ	The F_1 score is the harmonic mean of precision and recall
2È†ÖÂàÜÈ°ûÂô®„ÅØ2„Å§„ÅÆ„ÇØ„É©„Çπ„ÅÆÈñì„ÅÆÂå∫Âà•„Çí„Åô„Çã„Å†„Åë„Å†„Å£„Åü„ÅåÂ§ö„ÇØ„É©„ÇπÂàÜÈ°ûÂô®„ÅØ2„Å§‰ª•‰∏ä„ÅÆ„ÇØ„É©„Çπ„ÇíË¶ãÂàÜ„Åë„Çã„Åì„Å®„Çí„Åß„Åç„Çã	Whereas binary classifiers distinguish between two classes, multiclass classifiers can distinguish between more than two classes.
ÈÅ©ÂêàÁéá„Å®ÂÜçÁèæÁéá„ÅØ„Éà„É¨„Éº„Éâ„Ç™„Éï„ÅÆÈñ¢‰øÇ„Å´„ÅÇ„Çã„ÄÇ	This is called the precision/recall trade-off.
ÂàÜÈ°ûÂô®„ÅØ„ÄÅÂ≠¶ÁøíÊôÇ„Å´classes_Â±ûÊÄß„Å´ÂÄ§„ÅÆÈ†ÜÂ∫è„Åß„Çø„Éº„Ç≤„ÉÉ„Éà„ÇØ„É©„Çπ„ÅÆ„É™„Çπ„Éà„ÇíÊ†ºÁ¥ç„Åô„Çã„ÄÇ	When a classifier is trained, it stores the list of target classes in its classes_ attribute, ordered by value.
‰∏ÄÈÉ®„ÅÆÂ≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅØÂ≠¶Áøí„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆÈ†ÜÂ∫è„ÅÆÂΩ±Èüø„ÇíÂèó„Åë„ÄÅÂêå„Åò„Çà„ÅÜ„Å™„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅåÁ´ã„Å¶Á∂ö„Åë„Å´ÁôªÂ†¥„Åô„Çã„Å®ÊÄßËÉΩ„ÅåÂä£Âåñ„Åô„Çã„ÄÇ	Some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row.
Â§ö„ÇØ„É©„Çπ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶SDGClassifire„ÇíÂ≠¶Áøí„Åó„ÄÅ‰∫àÊ∏¨„Çí„Åô„Çã„ÅÆ„ÇÇÂêå„Åò„Åè„Çâ„ÅÑ„Å´Á∞°Âçò„Å†	Training an SDGClassifire on a multiclass dataset and using it to make prediction is just as easy
„Ç´„É©„Éº„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Åó„ÅüÊ∑∑ÂêåË°åÂàó„Çí‰Ωø„Åà„Å∞„ÄÅÂàÜÊûê„ÅåÂ§ßÂπÖ„Å´Ê•Ω„Å´„Å™„Çã„ÄÇ	A colored diagram of the confusion matrix is much easier to analyze.
„Åì„ÅÆÁ´†„Åß„ÅØ„ÄÅMNIST„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰Ωø„ÅÜ„ÄÇMNIST„ÅØ„ÄÅÈ´òÊ†°Áîü„ÇÑÁ±≥ÂõΩÂõΩÂã¢Ë™øÊüªÂ±Ä„ÅÆËÅ∑Âì°„ÅåÊâãÊõ∏„Åç„Åó„Åü70,000ÂÄã„ÅÆÊï∞Â≠óÁîªÂÉè„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„ÅÇ„Çã„ÄÇ	In this chapter we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau.
‰æã„Åà„Å∞„ÄÅÈñâ„Åò„ÅüËº™„ÅÆÊï∞„ÇíÊï∞„Åà„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÇíÊõ∏„Åè„ÅÆ„Åß„ÅÇ„Çã	For example, writing an algorithm to count the number of closed loops.
70,000ÂÄã„ÅÆÁîªÂÉè„Åå„ÅÇ„Çä„ÄÅÂÄã„ÄÖ„ÅÆÁîªÂÉè„Å´„ÅØ784ÂÄã„ÅÆÁâπÂæ¥Èáè„Åå„ÅÇ„Çã„ÄÇ	There are 70,000 images, and each image has 784 features.
ÂÖÉ„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„ÇíÂ∞ë„Åó‰∏¶Ë°åÁßªÂãï„Åó„Åü„Çä„ÄÅÂõûËª¢„Åó„Åü„Çä„Åó„ÅüÁîªÂÉè„Çí‰Ωú„Å£„Å¶Â≠¶Áøí„Çª„ÉÉ„Éà„ÇíË£ú„ÅÜÊñπ„Åå„ÅØ„Çã„Åã„Å´Á∞°Âçò„Å†„ÄÇ	A much simpler approach consists of augmenting the training set with slightly shifted and rotated variants of the training images.
„Åì„ÅÆ„Çà„ÅÜ„Å´Ë§áÊï∞„ÅÆ2ÂÄ§„Çø„Ç∞„ÇíÂá∫Âäõ„Åô„ÇãÂàÜÈ°û„Ç∑„Çπ„ÉÜ„É†„ÇíÂ§ö„É©„Éô„É´ÂàÜÈ°û„Ç∑„Çπ„ÉÜ„É†„Å®Âëº„Å∂„ÄÇ	Such a classification system that output multiple binary tags is called multilabel classification system
SGDClsassfier„ÅØ„ÄÅÂÄã„ÄÖ„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„Çπ„Å´ÂØæ„Åó„Å¶„ÄÅÊ±∫ÂÆöÈñ¢Êï∞„Å´Âü∫„Å•„ÅÑ„Å¶„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åó„ÄÅ„Åù„ÅÆ„Çπ„Ç≥„Ç¢„Åå„Åó„Åç„ÅÑÂÄ§„Çà„ÇäÈ´ò„Åë„Çå„Å∞„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅØÈôΩÊÄß„ÅÆ„ÇØ„É©„Çπ„Å´„ÄÅ„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞Èô∞ÊÄß„ÅÆ„ÇØ„É©„Çπ„Å´ÂàÜÈ°û„Åï„Çå„Çã„ÄÇ	For each instance, it computes a score based on a decision function.
ÂçòÁ¥î„Å´ÂÄã„ÄÖ„ÅÆ„É©„Éô„É´„ÅåÂ§ö„ÇØ„É©„Çπ„Åß„ÇÇËâØ„ÅÑÔºàË§áÊï∞„ÅÆÂÄ§„ÇíÊåÅ„Å£„Å¶ËâØ„ÅÑÔºâ„Å®„ÅÑ„ÅÜÂΩ¢„Å´Â§ö„É©„Éô„É´ÂàÜÈ°û„Çí‰∏ÄËà¨Âåñ„Åó„Åü„ÇÇ„ÅÆ„Å†„ÄÇ	It is a generalization of multilabel classification where each label can be multiclass(i.e., it can have more than two possible values)
„Åó„Åç„ÅÑÂÄ§„Çí‰∏ä„Åí„Çã„Å®ÂÜçÁèæÁéá„Åå‰∏ã„Åå„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åß„Åç„Çã„ÄÇ	This confirms that raising the threshold decreases recall.
ÂàÜÈ°û„Å®ÂõûÂ∏∞„ÅÆÂ¢ÉÁïå„ÅåÊõñÊòß„Å´„Å™„ÇãÂ†¥Âêà„Åå„ÅÇ„ÇãÔºé	The line between classification and regression is sometimes blurry.
Ê∑∑ÂêåË°åÂàó„ÅÆÂü∫Êú¨ÁöÑ„Å™ËÄÉ„ÅàÊñπ„ÅØ„ÄÅ„Åô„Åπ„Å¶„ÅÆA/B„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„Å´„Å§„ÅÑ„Å¶„ÄÅ„ÇØ„É©„ÇπA„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„Çπ„Åå„ÇØ„É©„ÇπB„Å´ÂàÜÈ°û„Åï„Çå„ÅüÂõûÊï∞„ÇíÊï∞„Åà„Çã„Å®„ÅÑ„ÅÜ„ÇÇ„ÅÆ„Åß„ÅÇ„Çã„ÄÇ	The general idea of a confusion matrix is to count the number of times instances of class A are classified as class B, for all A/B pairs.
ÂÆü„ÅØ„ÄÅÈÅ©ÂêàÁéá„ÅØ„Åó„Åç„ÅÑÂÄ§„Çí‰∏ä„Åí„Åü„Å®„Åç„Å´‰∏ã„Åå„Çã„Åì„Å®„Åå„Å®„Åç„Å©„Åç„ÅÇ„Çã„ÅÆ„Å†„ÄÇ	The reason is that precision may sometimes go down when you raise the threshold.
ÂÜçÁèæÁéá„Åå80%„ÇíË∂Ö„Åà„Åü„ÅÇ„Åü„Çä„Åã„ÇâÈÅ©ÂêàÁéá„ÅåÊÄ•ÈÄü„Å´ËêΩ„Å°„Å¶„ÅÑ„Åè„Åì„Å®„Åå„Çè„Åã„Çã„ÄÇ	You can see that precision really starts to fall sharply at around 80% recall.
Ê∑∑ÂêàË°åÂàó„ÇíÂàÜÊûê„Åô„Çã„Å®„ÄÅÂàÜÈ°ûÂô®„ÅÆÊîπÂñÑÊñπÊ≥ï„ÅÆ„Ç¢„Ç§„Éá„Ç¢„ÅåÁîü„Åæ„Çå„Çã„Åì„Å®„ÇÇÂ§ö„ÅÑÔºé	Analyzing the confusion matrix often gives you insights into ways to improve your classifier
Ê∑∑ÂêåË°åÂàó„ÅÆÂêÑË°å„ÅØÂÆüÈöõ„ÅÆ„ÇØ„É©„Çπ„ÄÅÂêÑÂàó„ÅØ‰∫àÊÉ≥„Åó„Åü„ÇØ„É©„Çπ„ÇíË°®„Åô„ÄÇ	Each row in a confusion matrix represents an actual class, while each column represents a predicted class.
ÂÜçÁèæÁéá„Åå‰∏ä„Åå„Çå„Å∞‰∏ä„Åå„Çã„Åª„Å©„ÄÅÂÅΩÈôΩÊÄß„Çí‰∏ä„Åå„Çã„ÅÆ„Åß„ÅÇ„Çã„ÄÇ	Once again there is a trade-off: the higher the recall, the more false positives the classifier produces.
„Åì„Çå„ÅßMatplotlib„Çí‰Ωø„Åà„Å∞„ÄÅFPR„Å´ÂØæ„Åô„ÇãTPR„Çí„Éó„É≠„ÉÉ„Éà„Åß„Åç„Çã„ÄÇ	Then you can plot the FPR against the TPR using Matplotlib.
precision_recall_curve()Èñ¢Êï∞„ÅØÂºïÊï∞„Å®„Åó„Å¶ÂÄã„ÄÖ„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆ„É©„Éô„É´„Å®„Çπ„Ç≥„Ç¢„ÇíÂèñ„Çã„ÅÆ„Åß„ÄÅ„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„ÉàÂàÜÈ°ûÂô®„ÇíÂ≠¶Áøí„Åó„ÄÅ„Åù„Çå„Çí‰Ωø„Å£„Å¶ÂêÑ„Ç§„É≥„Çπ„Çø„É≥„Çπ„Å´„Çπ„Ç≥„Ç¢„Çí‰∏é„Åà„Å™„Åë„Çå„Å∞„Å™„Çâ„Å™„ÅÑ„ÄÇ	the precision_recall_curve() function expects labels and scores for each instance, so we need to train the random forest classifier and make it assign a score to ecch instance.
„É©„É≥„ÉÄ„É†„Éï„Ç©„É¨„Çπ„Éà„ÅØFÂÄ§„ÇÑROCAUC„Çπ„Ç≥„Ç¢„Åß„ÇÇÂ§ßÂπÖ„Å´ÂÑ™„Çå„Å¶„ÅÑ„Çã„ÄÇ	It's F score and ROCAUC score are also significantly better.
„Åì„Çå„Çâ„ÅÆ‰øÇÊï∞„ÅØ„ÄÅÊúÄÈÅ©„Å™Œ±ÂÄ§„ÅåÂ≠¶Áøí„Çª„ÉÉ„Éà„Çµ„Ç§„Ç∫„Å´Â∑¶Âè≥„Åï„Çå„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã„Åü„ÇÅ„Å´ÈÅ∏„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ	These factors were chosen to ensure that the ooptimal a value is independent from the training set size.
0‰ª•Â§ñ„ÅÆÈáç„Åø„ÇíÊåÅ„Å§ÁâπÂæ¥Èáè„Åå„Åª„Å®„Çì„Å©„Å™„ÅÑ„É¢„Éá„É´	model with few nonzero feature weights.
„Åì„ÅÆ„Çà„ÅÜ„Å´„ÄÅÂÖ®‰Ωì„ÅÆÊúÄÈÅ©ÂÄ§„Å´„Éë„É©„É°„Éº„Çø„ÅåËøë„Å•„ÅÑ„Å¶„Åè„Çã„Å®„ÄÅ„ÄÅÂãæÈÖç„ÅØÁ∑©„ÇÑ„Åã„Å´„Å™„Çã„ÄÇ	As you can see, the gradients get smaller as the parameters approach the global optimum, so gradient descent naturally slows down.
„É¢„Éá„É´„ÅÆÂ≠¶Áøí„Å®„ÅØ„ÄÅ„É¢„Éá„É´„ÅåÂ≠¶Áøí„Çª„ÉÉ„Éà„Å´ÊúÄ„ÇÇ„Çà„ÅèÈÅ©Âêà„Åô„Çã„Çà„ÅÜ„Å™„É¢„Éá„É´„Éë„É©„É°„Éº„Çø„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„Å†	training a model means setting its parameters so that the model best fits the training set.
„Ç®„É©„Çπ„ÉÜ„Ç£„ÉÉ„ÇØ„Éç„ÉÉ„ÉàÂõûÂ∏∞„ÅØ„É™„ÉÉ„Ç∏ÂõûÂ∏∞„Å®„É©„ÉÉ„ÇΩÂõûÂ∏∞„ÅÆ‰∏≠Èñì„Åß„ÅÇ„Çã	elastic net regression is a middle ground between ridge regression and lasso regression.
@ÊºîÁÆóÂ≠ê„ÅØË°åÂàó„ÅÆ‰πóÁÆó„ÇíË°å„ÅÜ„ÄÇ	The @ operator performs matrix multiplication.
„Åì„Çå„ÅØ„ÄÅ„É¢„Éá„É´„ÅåÂ≠¶Áøí„Éá„Éº„Çø„ÇíÈÅéÂ≠¶Áøí„ÅóÂßã„ÇÅ„Åü„Åì„Å®„ÇíÁ§∫„Åô	this indicates that the model has started to overfit the training data.
„Åì„Çå„Åß„É≠„Ç∏„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÂõûÂ∏∞„ÅØ„ÄÅ2ÂÄ§ÂàÜÈ°ûÂô®„Å´„Å™„Çã„ÄÇ	This makes it a binary classifier.
„É≠„Ç∏„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÂõûÂ∏∞„ÅåÁ¢∫Áéá„ÇíÊé®ÂÆö„Åó„Å¶‰∫àÊ∏¨„Çí„Åô„Çã„Åì„Å®„ÅØ„Çè„Åã„Å£„Åü	now you konow how a logistic regression model estimates probabilities and makes predictions.
Â≠¶Áøí„Çª„ÉÉ„ÉàÂÖ®‰Ωì„Å´ÂØæ„Åô„ÇãÊêçÂ§±Èñ¢Êï∞„ÅØ„ÄÅÂçòÁ¥î„Å´„Åô„Åπ„Å¶„ÅÆÂ≠¶Áøí„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆ„Ç≥„Çπ„Éà„ÅÆÂπ≥Âùá„Åß„ÅÇ„Çã	the cost function over the whole training set is the average cost over all training instances.
„É¢„Éá„É´„ÇíÂ≠¶Áøí„Çª„ÉÉ„Éà„Å´„Éô„Çπ„Éà„Éï„Ç£„ÉÉ„Éà„Åï„Åõ„ÇãÁÇ∫„ÅÆ„É¢„Éá„É´„Éë„É©„É°„Éº„ÇøÔºà„Å§„Åæ„Çä„ÄÅÂ≠¶Áøí„Çª„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶ÊêçÂ§±Èñ¢Êï∞„ÅåÊúÄÂ∞è„Å´„Å™„Çã„Çà„ÅÜ„Å™„É¢„Éá„É´„Éë„É©„É°„Éº„ÇøÔºâ„ÇíÁõ¥Êé•Ë®àÁÆó„Åô„Çã„ÄåÈñâÂΩ¢Âºè„ÅÆÊñπÁ®ãÂºè„Äç„Çí‰Ωø„ÅÜÊñπÊ≥ï„ÄÇ	Using a "closed-form" equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimize the cost function over the training set).
È©ö„Åè„Åπ„Åç„Åì„Å®„Å´„ÄÅÁ∑öÂΩ¢„É¢„Éá„É´„ÅØÈùûÁ∑öÂΩ¢„Éá„Éº„Çø„Å´ÈÅ©Âêà„Åï„Åõ„Çâ„Çå„Çã„ÄÇ	Surprisingly, you can use a linear model to fit nonlinear data.
sklearn.base.clone()„Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åó„Åã„Ç≥„Éî„Éº„Åï„Çå„Å™„ÅÑ	sklearn.base.clone() only copies the model's hyperparameters.
„Çà„Çä‰∏ÄËà¨ÁöÑ„Å´„ÄÅÁ∑öÂΩ¢„É¢„Éá„É´„Å®„ÅØ„ÄÅÂºè4-1„Å´Á§∫„Åô„Çà„ÅÜ„Å´„ÄÅÂÖ•ÂäõÁâπÂæ¥Èáè„ÅÆÂä†ÈáçÁ∑èÂíå„Å´„ÄÅ„Éê„Ç§„Ç¢„ÇπÈ†ÖÔºàbias term„ÄÅÂàáÁâáÈ†ÖÔºöintercept term„Å®„ÇÇÂëº„Å∞„Çå„Çã„ÄÇÔºâ„Å®„ÅÑ„ÅÜÂÆöÊï∞„ÇíÂä†„Åà„Åü„ÇÇ„ÅÆ„Åß„ÅÇ„Çã„ÄÇ	More generally, a linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term), as shown in Equation 4-1.
„Çè„Åö„Åã„Å™„Åå„ÇâÈáç„Å™„ÇäÂêà„ÅÜÈÉ®ÂàÜ„Åå„ÅÇ„Çã„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Çà„ÅÜ	Notice that there is a bit of overlap.
X_poly„ÅØ„ÄÅX„ÅÆ„ÇÇ„Å®„ÇÇ„Å®„ÅÆÁâπÂæ¥Èáè„Å´„Åù„ÅÆ2‰πó„ÅÆÊñ∞„Åó„ÅÑÁâπÂæ¥Èáè„ÇíËøΩÂä†„Åó„Åü„ÇÇ„ÅÆ„Å´„Å™„Å£„Å¶„ÅÑ„Çã„ÄÇ	X_poly now contains the original feature of X plus the square of this fracture.
„Åî„Åè„Çè„Åö„Åã„Å™Â≠¶Áøí„Çª„ÉÉ„Éà„ÅßÂ≠¶Áøí„Åï„Çå„Åü„É¢„Éá„É´„Åß„ÅØ„ÄÅÂçÅÂàÜ„Å´Ê±éÂåñ„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊúÄÂàù„ÅÆ„ÅÜ„Å°„ÅØÊ§úË®ºË™§Â∑Æ„ÅØ„Åã„Å™„ÇäÂ§ß„Åç„ÅÑ„ÄÇ	When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite large.
Áâπ„Å´‰∏ÉÈáåÂ§ßË¶èÊ®°„Å™„Éü„Éã„Éê„ÉÉ„ÉÅ„Çí‰Ωø„Åà„Å∞Ë™§Â∑Æ„ÅåÂ∞è„Åï„Åè„Å™„Çã„ÄÇ	especially with fairly large mini-batches.
„Åì„Çå„Çí„ÇΩ„Éï„Éà„Éû„ÉÉ„ÇØ„ÇπÂõûÂ∏∞„ÅÇ„Çã„ÅÑ„ÅØ„ÄÅÂ§öÈ†Ö„É≠„Ç∏„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÂõûÂ∏∞„Å®Âëº„Å∂	this is calledsoftmax regression, or multinominal logistic regression.
Â≠¶Áøí„ÅÆÁõÆÁöÑ„ÅØ„ÄÅ„Çø„Éº„Ç≤„ÉÉ„Éà„ÇØ„É©„Çπ„ÇíÈ´ò„ÅÑÁ¢∫Áéá„ÅßÊé®ÂÆö„Åô„Çã„É¢„Éá„É´„Çí‰Ωú„Çã„Åì„Å®„Å†	the objective is to have a model that estimates a high probability for the target class.
‰∏ÄËà¨„Å´„ÄÅ„Ç§„É≥„Çπ„Çø„É≥„Çπ„Åå„Åù„ÅÆ„ÇØ„É©„Çπ„Å´Â±û„Åô„Çã„Åã„Å©„ÅÜ„Åã„Å´„Çà„Å£„Å¶1„Åæ„Åü„ÅØ0„Å´„Å™„Çã	In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not.
ÔºàÈÅ©Âêà„ÅÆË©ï‰æ°ÊñπÊ≥ï„Å®„Åó„Å¶„ÅØ„ÄÅÂ≠¶ÁøíÊõ≤Á∑ö„Å®„ÅÑ„ÅÜ„ÇÇ„ÅÆ„ÇÇ„ÅÇ„Çã„ÄÇÔºâÂ≠¶ÁøíÊõ≤Á∑ö„Å®„ÅØ„ÄÅÂ≠¶Áøí„Ç§„ÉÜ„É¨„Éº„Ç∑„Éß„É≥„ÅÆÈñ¢Êï∞„Å®„Åó„Å¶„É¢„Éá„É´„ÅÆË®ìÁ∑¥Ë™§Â∑Æ„Å®Ê§úË®ºË™§Â∑Æ„Çí„Éó„É≠„ÉÉ„Éà„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅÇ„Çã„ÄÇ	(Another way to tell is to look at the learning curves,)which are plots of the model‚Äôs training error and validation error as a function of the training iteration.
RidgeCV„ÇØ„É©„Çπ„ÇÇ„É™„ÉÉ„Ç∏ÂõûÂ∏∞„ÇíË°å„ÅÜ„Åå„ÄÅ„Åï„Çâ„Å´ÂÖ¨Â∑ÆÊ§úË®º„Çí‰Ωø„Å£„Åü„Éè„Ç§„Éë„É©„É°„Éº„Çø„ÅÆËá™ÂãïË™øÊï¥„ÇíË°å„ÅÜ„ÄÇ	The RidgeCV class also performs ridge regression, but it automatically tunes hyperparmeters using cross-validation.
ÁßÅÈÅî„ÅØ„ÄÅÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊúÄÂàù„ÅÆ„Éñ„É©„ÉÉ„ÇØ„Éú„ÉÉ„ÇØ„Çπ„Çí„Åì„ÅòÈñã„Åë„Åü„ÅÆ„Å†„ÄÇ	we've opened up the first machine learning black boxes!
Á∑öÂΩ¢ÂõûÂ∏∞„Å®ÂêåÊßò„Å´„ÄÅ„É™„ÉÉ„Ç∏ÂõûÂ∏∞„ÇâÈñâÂΩ¢Âºè„ÅÆÊñπÁ®ãÂºè„ÅÆË®àÁÆó„Åß„ÇÇÂãæÈÖçÈôç‰∏ãÊ≥ï„Å´„Çà„ÇãÂ≠¶Áøí„Åß„ÇÇÂÆüË°å„Åß„Åç„Çã„ÄÇ	As with linear regression, we can perform ridge regression either by computing a closed-form equation or by performing gradient descent.
„É¢„Éá„É´„ÅåÂ≠¶Áøí„Éá„Éº„Çø„Å´ÈÅéÂ∞èÈÅ©Âêà„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅÂ≠¶Áøí„Éá„Éº„Çø„ÇíËøΩÂä†„Åó„Å¶„ÇÇÊîπÂñÑ„Åó„Å™„ÅÑ„ÄÇ	If your model is underfitting the training data, adding more training examples will not help.
„Éé„Ç§„Ç∫„ÅÆ„Åä„Åã„Åí„Åß„ÇÇ„Å®„ÅÆÈñ¢Êï∞„ÅÆÊ≠£Á¢∫„Å™„Éë„É©„É°„Éº„Çø„ÅØÂæ©ÂÖÉ„Åß„Åç„Å™„Åè„Å™„Å£„Å¶„ÅÑ„Çã„ÄÇ	the noise made it impossible to recover the exact parameters of the original function.
„Éê„Ç§„Ç¢„Çπ„ÅÆÈ´ò„ÅÑ„É¢„Éá„É´„ÅØ„ÄÅÂ≠¶Áøí„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶ÈÅéÂ∞èÈÅ©Âêà„Åó„ÇÑ„Åô„ÅÑ„ÄÇ	A high-bias model is most likely to underfit the training data.
„Åì„ÅÆ„Çà„ÅÜ„Å™ÈÄÜË°åÂàóË®àÁÆó„ÅÆË®àÁÆóÈáè„ÅØ„ÄÅ‰∏ÄËà¨„Å´O(n^2.4)„Åã„ÇâO(n^3)„Åß„ÅÇ„Çã„ÄÇ	The computational complexity of inverting such a matrix is typically about O(n^2.4) to O(n^3).
Â≠¶ÁøíÁéá„ÇíÂ§ß„Åç„Åè„Åô„Çå„Å∞„Åô„Çã„Åª„Å©„ÄÅ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅØÁô∫Êï£„Åó„Å¶„Çà„ÅÑËß£„ÇíË¶ã„Å§„Åë„Çâ„Çå„Å™„Åè„Å™„Çã„ÄÇ	This might make the algorithm diverge, with larger and larger values, failing to find a good solution.
ÂãæÈÖçÈôç‰∏ãÊ≥ï„ÇíÂÆüË£Ö„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅÂÄã„ÄÖ„ÅÆ„É¢„Éá„É´„Éë„É©„É°„Éº„ÇøŒò_j„Å´„Å§„ÅÑ„Å¶ÊêçÂ§±Èñ¢Êï∞„ÅÆÂãæÈÖç„ÇíË®àÁÆó„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„ÄÇ	To implement gradient descent, you need to compute the gradient of the cost function with regard to each model parameter Œò_j.
ÂãæÈÖç„Éô„ÇØ„Éà„É´„ÇíÂæó„Åü„Å®„Åç„ÄÅÂÖ®‰Ωì„Å®„Åó„Å¶‰∏ä„ÇíÂêë„ÅÑ„Å¶„ÅÑ„Çã„Å™„Çâ„ÄÅÈÄÜ„ÅÆÊñπÂêë„Å´Âêë„Åã„Åà„Å∞‰∏ã„Å´Âêë„Åã„ÅÜ„ÄÇ	Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill.
„Éê„ÉÉ„ÉÅÂãæÈÖçÈôç‰∏ãÊ≥ï„ÅÆÊúÄÂ§ß„ÅÆÂïèÈ°å„ÅØ„ÄÅÂãæÈÖç„ÇíË®àÁÆó„Åô„Çã„Åü„ÇÅ„Å´ÂêÑ„Çπ„ÉÜ„ÉÉ„Éó„ÅßÂ≠¶Áøí„Çª„ÉÉ„Éà„ÇíÂÖ®ÈÉ®‰Ωø„ÅÜ„Åì„Å®„Å´„Çà„Çä„ÄÅÂ≠¶Áøí„Çª„ÉÉ„Éà„ÅåÂ§ß„Åç„ÅÑ„Å®„Åç„Å´„ÅØË®àÁÆóÈÄüÂ∫¶„ÅåÊ•µÁ´Ø„Å´ÈÅÖ„Åè„Å™„Çã„Åì„Å®„Å†„ÄÇ	The main problem with batch gradient descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.
„Å§„Åæ„Çä„ÄÅ„É©„É≥„ÉÄ„É†ÊÄß„ÅØÂ±ÄÊâÄÁöÑ„Å™ÊúÄÂ∞èÂÄ§„Åã„ÇâÈÄÉ„Çå„Çã„Åü„ÇÅ„Å´„ÅØ„Çà„ÅÑ„Åå„ÄÅÊúÄÂ∞èÂÄ§„Å´ËêΩ„Å°ÁùÄ„Åã„Å™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Å®„ÅÑ„ÅÜÁÇπ„Åß„ÅØ„Çà„Åè„Å™„ÅÑ„ÄÇ	Therefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum.
ÂãæÈÖçÈôç‰∏ãÊ≥ï(gradient descent)„ÅØ„ÄÅÈùûÂ∏∏„Å´Â∫É„ÅÑÁØÑÂõ≤„ÅÆÂïèÈ°å„ÅÆÊúÄÈÅ©Ëß£„ÇíË¶ã„Å§„Åë„Çâ„Çå„ÇãÊ±éÁî®ÊÄß„ÅåÈ´ò„ÅÑÊúÄÈÅ©Âåñ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å†„ÄÇ	Gradient descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems.
„Éê„ÉÉ„ÉÅÂãæÈÖçÈôç‰∏ãÊ≥ï„ÅÆ„Ç≥„Éº„Éâ„ÅåÂ≠¶Áøí„Çª„ÉÉ„ÉàÂÖ®‰Ωì„ÇíÂØæË±°„Å®„Åô„ÇãË®àÁÆó„Çí1,000ÂõûÁπ∞„ÇäËøî„Åó„Åü„ÅÆ„Å´ÂØæ„Åó„ÄÅ„Åì„ÅÆ„Ç≥„Éº„Éâ„ÅØÂ≠¶Áøí„Çª„ÉÉ„Éà„Çí50ÂõûÂá¶ÁêÜ„Åô„Çã„Å†„Åë„Åß„Åã„Å™„ÇäËâØ„ÅÑËß£„Å´„Åü„Å©„ÇäÁùÄ„Åè„ÄÇ	While the batch gradient descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times and reaches a pretty good solution:
scikit-learn„Çí‰Ωø„Å£„Å¶„ÅÑ„Å¶SGD„ÅßÁ∑öÂΩ¢ÂõûÂ∏∞„ÇíË°å„ÅÑ„Åü„ÅÑÂ†¥Âêà„Å´„ÅØ„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅßMSEÊêçÂ§±Èñ¢Êï∞„ÇíÊúÄÈÅ©Âåñ„Åô„ÇãSGDRegressor„ÇØ„É©„Çπ„Çí‰Ωø„Åà„Å∞„Çà„ÅÑ„ÄÇ	To perform linear regression using stochastic GD with Scikit-Learn, you can use the SGDRegressor class, which defaults to optimizing the MSE cost function.
„Éê„Ç§„Ç¢„ÇπÈ†Öb„ÅØ„Éû„Éº„Ç∏„É≥„ÅÆÂ§ß„Åç„Åï„Å´ÂΩ±Èüø„Çí‰∏é„Åà„Å™„ÅÑ„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Çà„ÅÜ„ÄÇ	Note that the bias term b has no influence on the size of the margin
QP„ÇΩ„É´„Éê„Éº„Çí‰Ωø„ÅÜ„ÅÆ„ÅØ„ÄÅSVM„ÇíÂ≠¶Áøí„Åô„Çã„Åü„ÇÅ„ÅÆÊñπÊ≥ï„ÅÆ1„Å§„Åß„ÅÇ„Çã„ÄÇ	Using a QP solver is one way to train an SVM.
Êù°‰ª∂‰ªò„ÅçÊúÄÈÅ©ÂåñÂïèÈ°å„ÅØ„ÄÅÁ∑ªÂØÜ„Å™Èñ¢ÈÄ£„ÇíÊåÅ„Å§Âà•„ÅÆÂïèÈ°å„Åß„ÇÇË°®Áèæ„Åß„Åç„Çã	Given a constrained optimization problem, know as the primal problem it is possible to express a different but closely related problem
SVM„ÅØË¶èÊ®°„Åå‰∏≠‰ª•‰∏ã(„Ç§„É≥„Çπ„Çø„É≥„ÇπÊï∞„ÅåÊï∞Áôæ„Åã„ÇâÊï∞ÂçÉ„ÅÆ„ÇÇ„ÅÆ)„ÅÆÈùûÁ∑öÂΩ¢„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÂäõ„ÇíÁô∫ÊèÆ„Åó„ÄÅÁâπ„Å´ÂàÜÈ°û„ÅÆ‰ªï‰∫ã„Å´Âêë„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ	SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especially for classification tasks.
Â≠¶Áøí„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆÊï∞„ÅåÁâπÂæ¥Èáè„ÅÆÊï∞„Çà„Çä„ÇÇÂ∞ë„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÂèåÂØæÂïèÈ°å„ÇíËß£„ÅèÊñπ„Åå‰∏ªÂïèÈ°å„ÇíËß£„Åè„Çà„Çä„ÇÇÊó©„ÅÑ„ÄÇ	The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features.
„Éè„Éº„Éâ„Éû„Éº„Ç∏„É≥ÂàÜÈ°û„Å´„ÅØ„ÄÅ„Éá„Éº„Çø„ÅåÁ∑öÂΩ¢ÂàÜÂâ≤„Åß„Åç„Çã„Å®„Åç„Åß„Å™„Åë„Çå„Å∞‰Ωø„Åà„Åö„ÄÅÂ§ñ„ÇåÂÄ§„Å´ÊïèÊÑü„Å´„Å™„Çä„Åô„Åé„Çã„Å®„ÅÑ„ÅÜ2„Å§„ÅÆÂ§ß„Åç„Å™ÂïèÈ°åÁÇπ„Åå„ÅÇ„Çã„ÄÇ	There are two main issues with hard margin classification. First, it only works if the data is linearly separable.
Second, it is sensitive to outliers.
Èñ¢Êï∞K(a,b)=(a^t b)^2„ÅØ„ÄÅ2Ê¨°ÂÖÉÂ§öÈ†ÖÂºè„Ç´„Éº„Éç„É´„Å®Âëº„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ	The function K(a,b) = (a b)2 is a second-degree polynomial kernel.
SVMÂàÜÈ°ûÂô®„ÅØÊñ∞Ë¶èÊÄßÊ§úÁü•„Å´„ÇÇ‰Ωø„Åà„Çã„ÄÇ	SVMs can also be used for novelty detection.
Âêå„Åò„Éà„É™„ÉÉ„ÇØ„Çí‰Ωø„Å£„Å¶„Éê„Ç§„Ç¢„ÇπÈ†Ö„ÅÆb„ÇÇË®àÁÆó„Åó„Å™„Åë„Çå„Å∞„Å™„Çâ„Å™„ÅÑ	you need to use the  same trick to compute the bias term b
SVM„É¢„Éá„É´„ÅåÈÅéÂ≠¶Áøí„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„Å´„ÅØ„ÄÅC„ÇíÂ∞è„Åï„Åè„Åó„Å¶Ê≠£ÂâáÂåñ„Åô„Çã„Å®„Çà„ÅÑ„ÄÇ	If your SVM model is overfitting, you can try regularizing it by reducing C.
Œµ„ÇíÂ∞è„Åï„Åè„Åô„Çã„Å®„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„Éº„ÅÆÊï∞„ÅåÂ¢ó„Åà„ÄÅ„É¢„Éá„É´„ÇíÊ≠£ÂâáÂåñ„Åô„Çã„ÄÇ	Reducing Œµ increases the number of support vectors, which regularizes the model.
Â§öÈ†ÖÂºèÁâπÂæ¥Èáè„ÅÆËøΩÂä†„ÅØÁ∞°Âçò„Å´ÂÆüË£Ö„Åß„Åç„ÄÅ„ÅÇ„Çâ„ÇÜ„ÇãÁ®ÆÈ°û„ÅÆÊ©üÊ¢∞Â≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É† (SVM„Å´Èôê„Çâ„Åö)„Åß„Åô„Å∞„Çâ„Åó„ÅèÊ©üËÉΩ„Åô„Çã„Åå„ÄÅÊ¨°Êï∞„Åå‰Ωé„ÅÑ„Å®ÈùûÂ∏∏„Å´Ë§áÈõë„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂá¶ÁêÜ„Åß„Åç„Åö„ÄÅÊ¨°Êï∞„ÅåÈ´ò„ÅÑ„Å®ÁâπÂæ¥Èáè„ÅåËÜ®Â§ß„Å™Êï∞„Å´„Å™„Å£„Å¶„É¢„Éá„É´„ÅåÈÅÖ„Åè„Å™„Çä„Åô„Åé„Çã„ÄÇ	Adding polynomial features is simple to implement and can work great with all sorts of machine learning algorithms (not just SVMs).
That said, at a low polynomial degree this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.
‰∏ÄÈÉ®„ÅÆ„Ç´„Éº„Éç„É´„ÅØ„ÄÅÁâπÂÆö„ÅÆ„Éá„Éº„ÇøÊßãÈÄ†„Å´ÁâπÂåñ„Åó„Å¶„ÅÑ„Çã„ÄÇ	Some kernel are specialized for specific data structures.
ÂÆüÈöõ„Å´È°û‰ººÊÄßÁâπÂæ¥Èáè„ÇíËøΩÂä†„Åó„Å™„Åè„Å¶„ÇÇ„ÄÅÂ§öÊï∞„ÅÆÈ°û‰ººÊÄßÁâπÂæ¥Èáè„ÇíËøΩÂä†„Åó„Åü„ÅÆ„Å®Âêå„ÅòÁµêÊûú„ÅåÂæó„Çâ„Çå„Çã„ÅÆ„Åß„ÅÇ„Çã„ÄÇ	making it possible to obtain a similar result as if you had added many similarity features, but without actually doing so.
ÈùûÁ∑öÂΩ¢ÂïèÈ°å„Å´„ÅØ„ÄÅÂÄã„ÄÖ„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅåÁâπÂÆö„ÅÆ„É©„É≥„Éâ„Éû„Éº„ÇØÔºàlandmarkÔºâ„Å´„Å©„ÅÆÁ®ãÂ∫¶Ëøë„ÅÑ„Åã„ÇíÊ∏¨ÂÆö„Åô„ÇãÈ°û‰ººÊÄßÈñ¢Êï∞ Ôºàsimlarity functionÔºâ„ÅÆË®àÁÆóÁµêÊûú„ÇíÁâπÂæ¥ÈÅº„Å®„Åó„Å¶ËøΩÂä†„Åô„Çã„Å®„ÅÑ„ÅÜÊñπÊ≥ï„Åß„ÇÇÂØæÂá¶„Åß„Åç„Çã(‰∫åÁ´†„ÅßÂú∞ÁêÜÁöÑÈ°û‰ººÊÄßÁâπÂæ¥Èáè„ÇíËøΩÂä†„Åó„Åü„Å®„Åç„Å®Âêå„Åò„Çà„ÅÜ„Å´Ôºâ„ÄÇ	Another technique to tackle nonlinear problems is to add features computed using a similarity function, which measures how much each instance resembles a particular landmark, as we did in Chapter 2 when we added the geographic similarity features.
Â≠¶Áøí„Çª„ÉÉ„Éà„ÅåÈùûÂ∏∏„Å´Â§ß„Åç„ÅÑÂ†¥Âêà„ÄÅÂêå„Åò„Çà„ÅÜ„Å´ÁâπÂæ¥ÈáèÊï∞„ÇÇÂ§ö„Åè„Å™„Å£„Å¶„Åó„Åæ„ÅÜ„ÄÇ	If your training set is very large, you end up with an equally large number of features.
Â≠¶Áøí„Å´„ÅØ„ÄÅÈÄêÊ¨°ÁöÑÂ≠¶Áøí„Å´ÂØæÂøú„Åó„ÄÅ„Åª„Å®„Çì„Å©„É°„É¢„É™„ÇíÊ∂àË≤ª„Åó„Å™„ÅÑSGD„Çí‰Ωø„ÅÜ„Åü„ÇÅ„ÄÅRAM„Å´ÂÖ•„ÇäÂàá„Çâ„Å™„ÅÑÂ§ßË¶èÊ®°„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„É¢„Éá„É´„ÇíÂ≠¶Áøí„Åô„Çã„Å®„Åç„Å´„ÇÇ‰Ωø„Åà„Çã„ÄÇ	For training it uses stochastic gradient descent,which allows incremental learning and uses little memory, so you can use it to train a model on a large dataset that does not fit in RAM.
Ê±∫ÂÆöÊú®„ÅØ„ÄÅÂõûÂ∏∞„ÅÆ„Çø„Çπ„ÇØ„ÇÇ„Åì„Å™„Åõ„Çã„ÄÇ	decision trees are also capable of performing regression tasks
Ê±∫ÂÆöÊú®„ÅØ„ÄÅÂõûÂ∏∞„Åß„ÇÇÂàÜÈ°û„ÅÆ„Å®„Åç„Å®Âêå„Åò„Çà„ÅÜ„Å´ÈÅéÂ≠¶Áøí„Åó„Åå„Å°„Å†„ÄÇ	just like for classification tasks, 
decision trees are prone to overfitting when dealing with regression tasks.
Ê±∫ÂÆöÊú®„ÅØÁêÜËß£„ÄÅËß£Èáà„Åó„ÇÑ„Åô„Åè„ÄÅ‰Ωø„ÅÑ„ÇÑ„Åô„Åè„ÄÅÊüîËªü„Åß„ÄÅÂº∑Âäõ„Åß„ÅÇ„Çã„ÄÇ	they are relatively easy to understand and interpret ,simple to use, versatile, and powerful.
„Çà„Çä‰∏ÄËà¨ÁöÑ„Å´Ë®Ä„ÅÜ„Å®„ÄÅÊ±∫ÂÆöÊú®„ÅÆÊúÄÂ§ß„ÅÆÂïèÈ°å„ÅØ„ÄÅ„Å∞„Çâ„Å§„Åç„ÅåÂ§ß„Åç„ÅÑ„Åì„Å®„Å†„ÄÇ	More generally, the main issue with decision trees is that they have quite a high variance.
CART„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅØ„ÄÅÂ≠¶Áøí„Çª„ÉÉ„Éà„ÅÆ2ÂàÜÂâ≤„Å´ÊàêÂäü„Åó„Åü„Çâ„Çµ„Éñ„Çª„ÉÉ„Éà„ÇíÂêå„ÅòË´ñÁêÜ„ÅßÂàÜÂâ≤„Åó„ÄÅ„Åù„ÅÆÊ¨°„ÅØ„Çµ„Éñ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅÆÂàÜÂâ≤„Å®„ÅÑ„ÅÜ„Çà„ÅÜ„Å´ÂÜçÂ∏∞ÁöÑ„Å´ÂàÜÂâ≤„Åô„Çã„ÄÇ	Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively.
„Ç®„É≥„Éà„É≠„Éî„Éº„ÅÆÊ¶ÇÂøµ„ÅØÁÜ±ÂäõÂ≠¶„ÅßÂàÜÂ≠ê„ÅÆ‰π±Èõë„Åï„ÅÆÊåáÊ®ô„Å®„Åó„Å¶‰Ωø„Å£„Åü„ÅÆ„ÅåÊúÄÂàù„Åß„ÅÇ„Çã„ÄÇ	The concept of entropy originated in thermodynamics as a measure of molecular disorder
„Åì„ÅÆÁ¢∫Áéá„ÅåÊåáÂÆö„Åï„Çå„Åü„Åó„Åç„ÅÑÂÄ§„Çà„ÇäÈ´ò„Åë„Çå„Å∞„ÄÅ„Éé„Éº„Éâ„ÅØ‰∏çË¶Å„Å†„Å®ËÄÉ„Åà„Çâ„Çå„ÄÅ„Åù„ÅÆÂ≠ê„ÅØÂâäÈô§„Åï„Çå„Çã„ÄÇ	If this probability is higher than a given threshold then the node is considered unnecessary and its children are deleted.
Ê±∫ÂÆöÊú®„ÅØÂº∑Âäõ„Åß„ÄÅË§áÈõë„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å´ÈÅ©Âêà„Åß„Åç„Çã„ÄÇ	They(decision trees) are powerful algorithms, capable of fitting complex datasets.
„É´„Éº„Éà„Éé„Éº„Éâ„Åã„Çâ„Çπ„Çø„Éº„Éà„Åô„Çã„ÄÇ	You start at the root node.
Ê≠£ÂâáÂåñ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅØ‰Ωø„ÅÜ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Çà„Å£„Å¶Áï∞„Å™„Çã„Åå„ÄÅ‰∏ÄËà¨„Å´Â∞ë„Å™„Åè„Å®„ÇÇÊ±∫ÂÆöÊú®„ÅÆÊ∑±„Åï„ÅÆ‰∏äÈôê„ÅØÂà∂Èôê„Åß„Åç„Çã„ÄÇ	The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the decision tree.
„Éé„Éº„Éâ„ÅÆÊù°‰ª∂„Å´ÂΩì„Å¶„ÅØ„Åæ„Çã„Åô„Åπ„Å¶„ÅÆÂ≠¶Áøí„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅåÂêå„Åò„ÇØ„É©„Çπ„Å´Â±û„Åô„Çã„Å™„Çâ„ÄÅ„Åù„ÅÆ„Éé„Éº„Éâ„ÅØ„ÄåÁ¥îÁ≤ã„Äç (gini=0)„Åß„ÅÇ„Çã„ÄÇ	a node is "pure" (gini=0) if all training instances it applies to belong to the same class.
    `;

      // Process the TSV data
      const flashcards = rawTsv
        .trim()
        .split("\n")
        .map((line) => {
          const parts = line.split("\t");
          if (parts.length < 2) return null;

          let jp = parts[0];
          const en = parts[1];

          // Cleanup Japanese text (remove )
          jp = jp.replace(/^\\s*/, "");

          return { jp: jp.trim(), en: en.trim() };
        })
        .filter((card) => card !== null);

      // --- State Management ---
      let currentIndex = 0;
      let mode = "jp_en"; // 'jp_en' or 'en_jp'
      let isRevealed = false;

      // --- DOM Elements ---
      const questionArea = document.getElementById("question-area");
      const answerArea = document.getElementById("answer-area");
      const answerContent = document.getElementById("answer-content");
      const progressIndicator = document.getElementById("progress-indicator");
      const btnJpEn = document.getElementById("mode-jp-en");
      const btnEnJp = document.getElementById("mode-en-jp");

      // --- TTS Setup ---
      function speak(text) {
        if (!window.speechSynthesis) return;

        // Cancel previous speech
        window.speechSynthesis.cancel();

        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = "en-US";

        // Attempt to find a high quality voice (Safari/iOS often has "Samantha" or "Fred" or system default)
        const voices = window.speechSynthesis.getVoices();
        const enVoice =
          voices.find(
            (v) => v.lang === "en-US" && v.name.includes("Samantha"),
          ) || voices.find((v) => v.lang.startsWith("en"));
        if (enVoice) utterance.voice = enVoice;

        window.speechSynthesis.speak(utterance);
      }

      // Ensure voices are loaded (safari quirk)
      window.speechSynthesis.onvoiceschanged = () => {
        // Just trigger a re-check internally if needed, but we check on click
      };

      function createTTSButton(text) {
        const btn = document.createElement("button");
        btn.className = "tts-btn";
        btn.innerHTML = `<svg viewBox="0 0 24 24"><path d="M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z"/></svg>`;
        btn.onclick = (e) => {
          e.stopPropagation(); // Don't trigger answer reveal
          speak(text);
        };
        return btn;
      }

      // --- Render Logic ---
      function renderCard() {
        if (flashcards.length === 0) return;

        const card = flashcards[currentIndex];

        // Reset answer state
        isRevealed = false;
        answerArea.classList.remove("revealed");
        questionArea.innerHTML = "";
        answerContent.innerHTML = "";

        if (mode === "jp_en") {
          // Question: Japanese, Answer: English
          questionArea.textContent = card.jp;

          // Answer Part
          const enText = document.createElement("div");
          enText.className = "text-with-icon";
          enText.textContent = card.en;
          answerContent.appendChild(enText);

          // TTS button next to Answer
          answerContent.appendChild(createTTSButton(card.en));
        } else {
          // Question: English, Answer: Japanese
          const enTextQ = document.createElement("div");
          enTextQ.className = "text-with-icon";
          enTextQ.textContent = card.en;
          questionArea.appendChild(enTextQ);

          // TTS button next to Question
          questionArea.appendChild(createTTSButton(card.en));

          // Answer Part
          answerContent.textContent = card.jp;
        }

        progressIndicator.textContent = `${currentIndex + 1} / ${flashcards.length}`;
      }

      // --- Interactions ---
      function revealAnswer() {
        if (!isRevealed) {
          isRevealed = true;
          answerArea.classList.add("revealed");
        }
      }

      function nextCard() {
        if (currentIndex < flashcards.length - 1) {
          currentIndex++;
        } else {
          currentIndex = 0; // Loop back
        }
        renderCard();
      }

      function prevCard() {
        if (currentIndex > 0) {
          currentIndex--;
        } else {
          currentIndex = flashcards.length - 1; // Loop to end
        }
        renderCard();
      }

      function setMode(newMode) {
        mode = newMode;
        if (mode === "jp_en") {
          btnJpEn.classList.add("active");
          btnEnJp.classList.remove("active");
        } else {
          btnEnJp.classList.add("active");
          btnJpEn.classList.remove("active");
        }
        renderCard();
      }

      // --- Initialization ---
      // Handle some messy lines in TSV might produce empty cards, filter handled above.
      renderCard();
    </script>
  </body>
</html>
